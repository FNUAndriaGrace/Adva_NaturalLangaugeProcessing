{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBw__kO4P8-h"
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KbGaKcUP8-j"
   },
   "source": [
    "#Assignment 1\n",
    "\n",
    "In this assignment, you will be asked to build an LSTM-based classifier that automatically determines whether a text was written by a human or by ChatGPT. You're dataset for this project is in the `csv` file `human_or_bot.csv`.\n",
    "\n",
    "Run the three tutorial notebooks in this folder on Google colab using GPU runtime. Make sure that you understand all of the content in these notebbooks and that they run successfully.\n",
    "\n",
    "In the current notebook, go through each cell containing a question and answer in an additional `Text` cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdN5hK6zbR3H"
   },
   "source": [
    "### Question 1\n",
    "\n",
    "When you run the LSTM sentiment classifier in `Assignment_1_LSTM_tutorial` you'll note that this classifier has a test accuracy of `0.0`. Why might that be? Hint: take a look at the original dataset and note how the train, test, and validation datasets have been generated.  Examine the distribution of labels in each dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Dataset:\n",
    "The original dataset used for training has a significant imbalance between positive and negative labels, the model may learn to predict only the majority class. For instance, if 90% of the training samples are positive, the model might predict all test samples as positive, resulting in poor accuracy on the minority class.\n",
    "\n",
    "### Incorrect Splitting of Data:\n",
    "The train, test, and validation datasets are generated in such a way that they do not represent the overall distribution of labels accurately, the model may not generalize well. For example, if all positive samples are in the training set and all negative samples are in the test set, the model will achieve high accuracy on the training set but will perform poorly on the test set.\n",
    "\n",
    "### Data Leakage:\n",
    "There is a leakage between the datasets (e.g., if the same samples or very similar samples are present in both the training and testing datasets), this could lead to misleading results. The model may perform well on the training data but fails to generalize to unseen data.\n",
    "\n",
    "### Overfitting:\n",
    "The model is overfitting to the training data (especially if the training set is small), it may not perform well on the test set, particularly if the test set includes samples that differ from the training set.\n",
    "\n",
    "### Incorrect Label Encoding:\n",
    "There are issues with how the labels are encoded (e.g., mislabeling or errors in the mapping), the model could misinterpret the classes, leading to incorrect predictions.\n",
    "\n",
    "### Steps to Diagnose the Issue\n",
    "\n",
    "1. Check the distribution of the labels in the training, validation, and test datasets. Use pandas to visualize the counts of each class.\n",
    "\n",
    "2. Check Data Splitting Logic:\n",
    "\n",
    "3. Evaluate Model Performance on Train Set:\n",
    "\n",
    "4. Hyperparameter Tuning:\n",
    "\n",
    "### Conclusion\n",
    "Test accuracy of 0.0 indicates a significant issue in either the dataset or the model training process. By closely examining the data distribution and splitting, you can identify the root cause of the problem and take appropriate corrective actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtZiculkb6an"
   },
   "source": [
    "### Question 2\n",
    "\n",
    "After you've identifed the problem with the original run of the LSTM model in the `Assignment_1_LSTM_tutorial` notebook, run the training and test cells again and report the new accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'review', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample data loading\n",
    "# Replace 'your_dataset.csv' with your actual dataset path\n",
    "# Example dataset structure:\n",
    "#   - text: the content of the review\n",
    "#   - sentiment: the corresponding sentiment label (positive/negative)\n",
    "data = pd.read_csv('imdb_small.csv')\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2, Val Loss: 0.697614\n",
      "Epoch: 2/2, Val Loss: 0.690841\n",
      "Test loss: 0.694\n",
      "Test accuracy: 0.460\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('imdb_small.csv')\n",
    "\n",
    "# Basic preprocessing\n",
    "data['review'] = data['review'].str.lower()  # Convert to lowercase\n",
    "data['sentiment'] = data['sentiment'].map({'positive': 1, 'negative': 0})  # Encode sentiments\n",
    "\n",
    "# Split data into features and labels\n",
    "texts = data['review'].values\n",
    "labels = data['sentiment'].values\n",
    "\n",
    "# Tokenization and padding\n",
    "vectorizer = CountVectorizer(max_features=5000, token_pattern=r'\\b\\w+\\b')\n",
    "X = vectorizer.fit_transform(texts).toarray()  # Convert to numerical representation\n",
    "vocab_size = len(vectorizer.vocabulary_) + 1  # +1 for padding\n",
    "\n",
    "# Hyperparameters\n",
    "output_size = 1\n",
    "embedding_dim = 300  # Adjusted for smaller embeddings\n",
    "hidden_dim = 128     # Reduced to prevent overfitting\n",
    "n_layers = 2\n",
    "dropout_prob = 0.5\n",
    "lr = 0.001\n",
    "epochs = 2  # Increased epochs for better training\n",
    "batch_size = 5  # Define a batch size as needed\n",
    "clip = 5  # Gradient clipping\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# Create DataLoader\n",
    "train_size = int(0.8 * len(X))\n",
    "valid_size = int(0.1 * len(X))\n",
    "test_size = len(X) - train_size - valid_size\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "train_dataset = SentimentDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "valid_dataset = SentimentDataset(torch.tensor(X_valid, dtype=torch.float32), torch.tensor(y_valid, dtype=torch.float32))\n",
    "test_dataset = SentimentDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define the LSTM model\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.Linear(16, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        embedd = self.embedding(x.long())\n",
    "        lstm_out, hidden = self.lstm(embedd.view(batch_size, -1, self.embedding.embedding_dim), hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        sig_out = self.sigmoid(out)\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]\n",
    "        return sig_out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        if torch.cuda.is_available():\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n",
    "\n",
    "# Initialize the model\n",
    "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout_prob)\n",
    "\n",
    "# Check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "# Move model to GPU, if available\n",
    "if train_on_gpu:\n",
    "    net.cuda()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# Training the model\n",
    "net.train()\n",
    "for e in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        batch_size = inputs.size(0)  # Use the actual batch size\n",
    "        h = net.init_hidden(batch_size)  # Initialize hidden state with the correct batch size\n",
    "\n",
    "        if train_on_gpu:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        h = tuple([each.data for each in h])  # Detach hidden states\n",
    "        optimizer.zero_grad()  # Zero out accumulated gradients\n",
    "        output, h = net(inputs, h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loss\n",
    "    val_losses = []\n",
    "    net.eval()\n",
    "    for inputs, labels in valid_loader:\n",
    "        batch_size = inputs.size(0)  # Use the actual batch size for validation\n",
    "        val_h = net.init_hidden(batch_size)\n",
    "\n",
    "        if train_on_gpu:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "        output, val_h = net(inputs, val_h)\n",
    "        val_loss = criterion(output.squeeze(), labels.float())\n",
    "        val_losses.append(val_loss.item())\n",
    "    \n",
    "    print(f\"Epoch: {e+1}/{epochs}, Val Loss: {np.mean(val_losses):.6f}\")\n",
    "\n",
    "# Testing the model\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "h = net.init_hidden(batch_size)\n",
    "net.eval()\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    batch_size = inputs.size(0)  # Use the actual batch size for testing\n",
    "    h = net.init_hidden(batch_size)  # Initialize hidden state with the correct batch size\n",
    "    h = tuple([each.data for each in h])\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    output, h = net(inputs, h)\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred = torch.round(output.squeeze())\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy()) if train_on_gpu else np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "# Calculate and print average test loss and accuracy\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc = num_correct / len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "303P2jp1d5mL"
   },
   "source": [
    "### Question 3\n",
    "\n",
    "Try to improve the accuracy of the LSTM model by tuning the following hyper-parameters\n",
    "\n",
    "- Learning rate (this is the `lr` variable in the __Training__ cell)\n",
    "- Dropout probability (`dropout_prob` in the __Instantiate the model with hyperparameters__ cell)\n",
    "- Number of epochs (This is the `epochs` variable in the __Training__  cell)\n",
    "\n",
    "Choose at least 4 values (2 lower and 2 higher) for each parameter and answer the following questions\n",
    "- Did the accuracy increase or decrease?\n",
    "- Based on your understanding of the hyperparameter, why do you think it increased or decreased?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.0001, Dropout: 0.1, Epochs: 1, Test Accuracy: 0.4600\n",
      "Learning Rate: 0.0001, Dropout: 0.1, Epochs: 2, Test Accuracy: 0.5600\n",
      "Learning Rate: 0.0001, Dropout: 0.1, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.0001, Dropout: 0.1, Epochs: 3, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.0001, Dropout: 0.3, Epochs: 1, Test Accuracy: 0.4500\n",
      "Learning Rate: 0.0001, Dropout: 0.3, Epochs: 2, Test Accuracy: 0.4600\n",
      "Learning Rate: 0.0001, Dropout: 0.3, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.0001, Dropout: 0.3, Epochs: 3, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.0001, Dropout: 0.5, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.0001, Dropout: 0.5, Epochs: 2, Test Accuracy: 0.4600\n",
      "Learning Rate: 0.0001, Dropout: 0.5, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.0001, Dropout: 0.5, Epochs: 3, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.0001, Dropout: 0.7, Epochs: 1, Test Accuracy: 0.5500\n",
      "Learning Rate: 0.0001, Dropout: 0.7, Epochs: 2, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.0001, Dropout: 0.7, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.0001, Dropout: 0.7, Epochs: 3, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.001, Dropout: 0.1, Epochs: 1, Test Accuracy: 0.4600\n",
      "Learning Rate: 0.001, Dropout: 0.1, Epochs: 2, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.001, Dropout: 0.1, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.001, Dropout: 0.1, Epochs: 3, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.001, Dropout: 0.3, Epochs: 1, Test Accuracy: 0.4600\n",
      "Learning Rate: 0.001, Dropout: 0.3, Epochs: 2, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.001, Dropout: 0.3, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.001, Dropout: 0.3, Epochs: 3, Test Accuracy: 0.4600\n",
      "Learning Rate: 0.001, Dropout: 0.5, Epochs: 1, Test Accuracy: 0.4600\n",
      "Learning Rate: 0.001, Dropout: 0.5, Epochs: 2, Test Accuracy: 0.4600\n",
      "Learning Rate: 0.001, Dropout: 0.5, Epochs: 1, Test Accuracy: 0.4600\n",
      "Learning Rate: 0.001, Dropout: 0.5, Epochs: 3, Test Accuracy: 0.4600\n",
      "Learning Rate: 0.001, Dropout: 0.7, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.001, Dropout: 0.7, Epochs: 2, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.001, Dropout: 0.7, Epochs: 1, Test Accuracy: 0.4600\n",
      "Learning Rate: 0.001, Dropout: 0.7, Epochs: 3, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.01, Dropout: 0.1, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.01, Dropout: 0.1, Epochs: 2, Test Accuracy: 0.4600\n",
      "Learning Rate: 0.01, Dropout: 0.1, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.01, Dropout: 0.1, Epochs: 3, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.01, Dropout: 0.3, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.01, Dropout: 0.3, Epochs: 2, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.01, Dropout: 0.3, Epochs: 1, Test Accuracy: 0.4600\n",
      "Learning Rate: 0.01, Dropout: 0.3, Epochs: 3, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.01, Dropout: 0.5, Epochs: 1, Test Accuracy: 0.4600\n",
      "Learning Rate: 0.01, Dropout: 0.5, Epochs: 2, Test Accuracy: 0.4600\n",
      "Learning Rate: 0.01, Dropout: 0.5, Epochs: 1, Test Accuracy: 0.4600\n",
      "Learning Rate: 0.01, Dropout: 0.5, Epochs: 3, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.01, Dropout: 0.7, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.01, Dropout: 0.7, Epochs: 2, Test Accuracy: 0.4600\n",
      "Learning Rate: 0.01, Dropout: 0.7, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.01, Dropout: 0.7, Epochs: 3, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.1, Dropout: 0.1, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.1, Dropout: 0.1, Epochs: 2, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.1, Dropout: 0.1, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.1, Dropout: 0.1, Epochs: 3, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.1, Dropout: 0.3, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.1, Dropout: 0.3, Epochs: 2, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.1, Dropout: 0.3, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.1, Dropout: 0.3, Epochs: 3, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.1, Dropout: 0.5, Epochs: 1, Test Accuracy: 0.4600\n",
      "Learning Rate: 0.1, Dropout: 0.5, Epochs: 2, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.1, Dropout: 0.5, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.1, Dropout: 0.5, Epochs: 3, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.1, Dropout: 0.7, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.1, Dropout: 0.7, Epochs: 2, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.1, Dropout: 0.7, Epochs: 1, Test Accuracy: 0.5400\n",
      "Learning Rate: 0.1, Dropout: 0.7, Epochs: 3, Test Accuracy: 0.5400\n",
      "\n",
      "Results Summary:\n",
      "    Learning Rate  Dropout  Epochs  Test Accuracy\n",
      "0          0.0001      0.1       1           0.46\n",
      "1          0.0001      0.1       2           0.56\n",
      "2          0.0001      0.1       1           0.54\n",
      "3          0.0001      0.1       3           0.54\n",
      "4          0.0001      0.3       1           0.45\n",
      "..            ...      ...     ...            ...\n",
      "59         0.1000      0.5       3           0.54\n",
      "60         0.1000      0.7       1           0.54\n",
      "61         0.1000      0.7       2           0.54\n",
      "62         0.1000      0.7       1           0.54\n",
      "63         0.1000      0.7       3           0.54\n",
      "\n",
      "[64 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter values to test\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "dropout_probs = [0.1, 0.3, 0.5, 0.7]\n",
    "num_epochs = [1, 2, 1, 3]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Running experiments\n",
    "for lr in learning_rates:\n",
    "    for dropout in dropout_probs:\n",
    "        for epoch in num_epochs:\n",
    "            # Initialize the model with the current hyperparameters\n",
    "            net = SentimentLSTM(vocab_size, output_size=1, embedding_dim=300, hidden_dim=128, n_layers=2, drop_prob=dropout)\n",
    "\n",
    "            # Check if CUDA is available\n",
    "            train_on_gpu = torch.cuda.is_available()\n",
    "            if train_on_gpu:\n",
    "                net.cuda()\n",
    "\n",
    "            # Loss and optimizer\n",
    "            criterion = nn.BCELoss()\n",
    "            optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "            # Training the model\n",
    "            for e in range(epoch):\n",
    "                net.train()  # Set model to training mode\n",
    "                for inputs, labels in train_loader:\n",
    "                    batch_size = inputs.size(0)  # Use the actual batch size\n",
    "                    h = net.init_hidden(batch_size)  # Initialize hidden state\n",
    "\n",
    "                    if train_on_gpu:\n",
    "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                    \n",
    "                    h = tuple([each.data for each in h])  # Detach hidden states\n",
    "                    optimizer.zero_grad()  # Zero out accumulated gradients\n",
    "                    output, h = net(inputs, h)\n",
    "                    loss = criterion(output.squeeze(), labels.float())\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(net.parameters(), 5)  # Gradient clipping\n",
    "                    optimizer.step()\n",
    "\n",
    "            # Testing the model and calculate accuracy\n",
    "            net.eval()  # Set model to evaluation mode\n",
    "            test_losses = []\n",
    "            num_correct = 0\n",
    "\n",
    "            for inputs, labels in test_loader:\n",
    "                batch_size = inputs.size(0)  # Use the actual batch size for testing\n",
    "                h = net.init_hidden(batch_size)  # Initialize hidden state\n",
    "                h = tuple([each.data for each in h])\n",
    "                \n",
    "                if train_on_gpu:\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                \n",
    "                output, h = net(inputs, h)\n",
    "                test_loss = criterion(output.squeeze(), labels.float())\n",
    "                test_losses.append(test_loss.item())\n",
    "                pred = torch.round(output.squeeze())\n",
    "                correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "                correct = np.squeeze(correct_tensor.cpu().numpy()) if train_on_gpu else np.squeeze(correct_tensor.numpy())\n",
    "                num_correct += np.sum(correct)\n",
    "\n",
    "            # Calculate and store results\n",
    "            test_acc = num_correct / len(test_loader.dataset)\n",
    "            results.append((lr, dropout, epoch, test_acc))\n",
    "            print(f\"Learning Rate: {lr}, Dropout: {dropout}, Epochs: {epoch}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Convert results to a DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results, columns=['Learning Rate', 'Dropout', 'Epochs', 'Test Accuracy'])\n",
    "print(\"\\nResults Summary:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abm8X0pQcLsh"
   },
   "source": [
    "### Question 4\n",
    "\n",
    "In the current notebook, cut and paste the LSTM code from `Assignment_1_LSTM_tutorial` and train the model on `human_or_bot.csv` (make sure that you create the train/test/validation sets!).\n",
    "\n",
    "What is the test accuracy of your model?\n",
    "\n",
    "Next, repeat the hyper-paraemter tuning you performed as part of Question 3, this time for your human vs. bot classification model. Again, choose 4 values for each hyper-paraemter, report out the change in test set performance, and the reason for that change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflowNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/ed/b6/62345568cd07de5d9254fcf64d7e44aacbb6abde11ea953b3cb320e58d19/tensorflow-2.17.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow-2.17.0-cp311-cp311-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting tensorflow-intel==2.17.0 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-intel==2.17.0 from https://files.pythonhosted.org/packages/66/03/5c447feceb72f5a38ac2aa79d306fa5b5772f982c2b480c1329c7e382900/tensorflow_intel-2.17.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow_intel-2.17.0-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Obtaining dependency information for astunparse>=1.6.0 from https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=24.3.25 from https://files.pythonhosted.org/packages/41/f0/7e988a019bc54b2dbd0ad4182ef2d53488bb02e58694cd79d61369e85900/flatbuffers-24.3.25-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Obtaining dependency information for gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 from https://files.pythonhosted.org/packages/a3/61/8001b38461d751cd1a0c3a6ae84346796a5758123f3ed97a1b121dfbf4f3/gast-0.6.0-py3-none-any.whl.metadata\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Obtaining dependency information for google-pasta>=0.1.1 from https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Obtaining dependency information for h5py>=3.10.0 from https://files.pythonhosted.org/packages/d8/5e/b7b83cfe60504cc4d24746aed04353af7ea8ec104e597e5ae71b8d0390cb/h5py-3.11.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading h5py-3.11.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/0b/2d/3f480b1e1d31eb3d6de5e3ef641954e5c67430d5ac93b7fa7e07589576c7/libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Obtaining dependency information for ml-dtypes<0.5.0,>=0.3.1 from https://files.pythonhosted.org/packages/e8/d3/ddfd9878b223b3aa9a930c6100a99afca5cfab7ea703662e00323acb7568/ml_dtypes-0.4.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading ml_dtypes-0.4.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Obtaining dependency information for opt-einsum>=2.3.2 from https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl.metadata\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (23.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/a7/ad/bf3f358e90b7e70bf7fb520702cb15307ef268262292d3bdb16ad8ebc815/protobuf-4.25.5-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.5-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Obtaining dependency information for termcolor>=1.1.0 from https://files.pythonhosted.org/packages/d9/5f/8c716e47b3a50cbd7c146f45881e11d9414def768b7cd9c5e6650ec2a80a/termcolor-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Collecting keras>=3.2.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Obtaining dependency information for keras>=3.2.0 from https://files.pythonhosted.org/packages/a3/a4/101f0f3c0b057ce150af0e8493ab7fc10b98b066b7bd81ab01e96038a268/keras-3.5.0-py3-none-any.whl.metadata\n",
      "  Downloading keras-3.5.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/ac/4e/9566a313927be582ca99455a9523a097c7888fc819695bdc08415432b202/tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: rich in c:\\users\\acer\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.7.1)\n",
      "Collecting namex (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\n",
      "  Obtaining dependency information for namex from https://files.pythonhosted.org/packages/73/59/7854fbfb59f8ae35483ce93493708be5942ebb6328cd85b3a609df629736/namex-0.0.8-py3-none-any.whl.metadata\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\n",
      "  Obtaining dependency information for optree from https://files.pythonhosted.org/packages/12/26/ce14a11c328bfcf29341da42342cc48e4d65fb44e4e559c9e4036a88b750/optree-0.12.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading optree-0.12.1-cp311-cp311-win_amd64.whl.metadata (48 kB)\n",
      "     ---------------------------------------- 0.0/48.7 kB ? eta -:--:--\n",
      "     --------------------------------- ------ 41.0/48.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 48.7/48.7 kB 610.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.17.0-cp311-cp311-win_amd64.whl (2.0 kB)\n",
      "Downloading tensorflow_intel-2.17.0-cp311-cp311-win_amd64.whl (385.0 MB)\n",
      "   ---------------------------------------- 0.0/385.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/385.0 MB 10.0 MB/s eta 0:00:39\n",
      "   ---------------------------------------- 1.5/385.0 MB 16.3 MB/s eta 0:00:24\n",
      "   ---------------------------------------- 3.1/385.0 MB 21.9 MB/s eta 0:00:18\n",
      "    --------------------------------------- 4.9/385.0 MB 26.1 MB/s eta 0:00:15\n",
      "    --------------------------------------- 6.8/385.0 MB 28.9 MB/s eta 0:00:14\n",
      "    --------------------------------------- 8.5/385.0 MB 28.6 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 9.8/385.0 MB 28.5 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 11.8/385.0 MB 36.4 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 13.7/385.0 MB 38.6 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 15.7/385.0 MB 36.3 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 17.7/385.0 MB 40.9 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 19.6/385.0 MB 40.9 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 21.6/385.0 MB 40.9 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 23.3/385.0 MB 40.9 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 24.8/385.0 MB 38.5 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 26.6/385.0 MB 38.5 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 28.6/385.0 MB 38.5 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 30.2/385.0 MB 36.3 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 31.4/385.0 MB 34.4 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 33.4/385.0 MB 34.6 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 35.2/385.0 MB 36.4 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 37.1/385.0 MB 36.4 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 38.9/385.0 MB 36.4 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 40.3/385.0 MB 36.4 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 42.1/385.0 MB 36.4 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 43.7/385.0 MB 34.4 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 45.5/385.0 MB 38.5 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 47.5/385.0 MB 36.4 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 49.4/385.0 MB 36.4 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 50.9/385.0 MB 38.6 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 52.7/385.0 MB 36.3 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 53.8/385.0 MB 36.4 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 56.2/385.0 MB 36.4 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 58.1/385.0 MB 36.4 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 59.9/385.0 MB 36.4 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 61.8/385.0 MB 38.5 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 63.8/385.0 MB 38.5 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 65.6/385.0 MB 38.6 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 67.7/385.0 MB 38.6 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 69.5/385.0 MB 40.9 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 71.5/385.0 MB 38.5 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 73.4/385.0 MB 40.9 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 74.2/385.0 MB 40.9 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 74.2/385.0 MB 40.9 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 74.2/385.0 MB 40.9 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 74.9/385.0 MB 27.3 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 76.6/385.0 MB 25.2 MB/s eta 0:00:13\n",
      "   -------- ------------------------------- 78.5/385.0 MB 24.2 MB/s eta 0:00:13\n",
      "   -------- ------------------------------- 81.3/385.0 MB 26.2 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 83.3/385.0 MB 26.2 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 85.2/385.0 MB 50.4 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 87.2/385.0 MB 43.5 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 89.1/385.0 MB 43.5 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 91.1/385.0 MB 40.9 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 92.9/385.0 MB 40.9 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 94.7/385.0 MB 40.9 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 94.7/385.0 MB 40.9 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 98.0/385.0 MB 38.5 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 99.7/385.0 MB 34.6 MB/s eta 0:00:09\n",
      "   ---------- ---------------------------- 101.8/385.0 MB 38.5 MB/s eta 0:00:08\n",
      "   ---------- ---------------------------- 103.2/385.0 MB 36.3 MB/s eta 0:00:08\n",
      "   ---------- ---------------------------- 105.2/385.0 MB 46.9 MB/s eta 0:00:06\n",
      "   ---------- ---------------------------- 106.7/385.0 MB 38.5 MB/s eta 0:00:08\n",
      "   ----------- --------------------------- 108.6/385.0 MB 38.6 MB/s eta 0:00:08\n",
      "   ----------- --------------------------- 110.7/385.0 MB 40.9 MB/s eta 0:00:07\n",
      "   ----------- --------------------------- 112.5/385.0 MB 38.5 MB/s eta 0:00:08\n",
      "   ----------- --------------------------- 114.4/385.0 MB 38.6 MB/s eta 0:00:08\n",
      "   ----------- --------------------------- 116.4/385.0 MB 43.5 MB/s eta 0:00:07\n",
      "   ----------- --------------------------- 118.0/385.0 MB 38.6 MB/s eta 0:00:07\n",
      "   ------------ -------------------------- 119.8/385.0 MB 38.5 MB/s eta 0:00:07\n",
      "   ------------ -------------------------- 121.7/385.0 MB 38.5 MB/s eta 0:00:07\n",
      "   ------------ -------------------------- 123.6/385.0 MB 38.5 MB/s eta 0:00:07\n",
      "   ------------ -------------------------- 125.7/385.0 MB 38.5 MB/s eta 0:00:07\n",
      "   ------------ -------------------------- 127.5/385.0 MB 38.5 MB/s eta 0:00:07\n",
      "   ------------- ------------------------- 129.6/385.0 MB 40.9 MB/s eta 0:00:07\n",
      "   ------------- ------------------------- 131.4/385.0 MB 38.5 MB/s eta 0:00:07\n",
      "   ------------- ------------------------- 133.3/385.0 MB 38.5 MB/s eta 0:00:07\n",
      "   ------------- ------------------------- 135.0/385.0 MB 40.9 MB/s eta 0:00:07\n",
      "   ------------- ------------------------- 136.9/385.0 MB 40.9 MB/s eta 0:00:07\n",
      "   -------------- ------------------------ 139.0/385.0 MB 38.6 MB/s eta 0:00:07\n",
      "   -------------- ------------------------ 140.9/385.0 MB 38.6 MB/s eta 0:00:07\n",
      "   -------------- ------------------------ 142.7/385.0 MB 40.9 MB/s eta 0:00:06\n",
      "   -------------- ------------------------ 144.8/385.0 MB 38.5 MB/s eta 0:00:07\n",
      "   -------------- ------------------------ 146.1/385.0 MB 38.5 MB/s eta 0:00:07\n",
      "   -------------- ------------------------ 147.6/385.0 MB 38.6 MB/s eta 0:00:07\n",
      "   --------------- ----------------------- 148.1/385.0 MB 32.7 MB/s eta 0:00:08\n",
      "   --------------- ----------------------- 150.9/385.0 MB 34.4 MB/s eta 0:00:07\n",
      "   --------------- ----------------------- 152.1/385.0 MB 32.8 MB/s eta 0:00:08\n",
      "   --------------- ----------------------- 154.0/385.0 MB 32.8 MB/s eta 0:00:08\n",
      "   --------------- ----------------------- 155.3/385.0 MB 31.2 MB/s eta 0:00:08\n",
      "   --------------- ----------------------- 157.3/385.0 MB 32.8 MB/s eta 0:00:07\n",
      "   ---------------- ---------------------- 158.8/385.0 MB 38.6 MB/s eta 0:00:06\n",
      "   ---------------- ---------------------- 160.5/385.0 MB 34.4 MB/s eta 0:00:07\n",
      "   ---------------- ---------------------- 161.9/385.0 MB 34.4 MB/s eta 0:00:07\n",
      "   ---------------- ---------------------- 163.3/385.0 MB 32.7 MB/s eta 0:00:07\n",
      "   ---------------- ---------------------- 165.9/385.0 MB 36.4 MB/s eta 0:00:07\n",
      "   ---------------- ---------------------- 167.8/385.0 MB 38.6 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 169.7/385.0 MB 38.5 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 170.7/385.0 MB 40.9 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 171.7/385.0 MB 34.4 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 172.4/385.0 MB 36.3 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 173.9/385.0 MB 31.2 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 177.0/385.0 MB 34.4 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 178.2/385.0 MB 31.2 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 179.7/385.0 MB 29.7 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 181.7/385.0 MB 34.4 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 183.7/385.0 MB 40.9 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 185.7/385.0 MB 38.5 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 187.5/385.0 MB 38.6 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 189.6/385.0 MB 38.6 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 191.2/385.0 MB 40.9 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 193.2/385.0 MB 40.9 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 195.1/385.0 MB 38.5 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 196.9/385.0 MB 40.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 199.0/385.0 MB 38.6 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 201.0/385.0 MB 40.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 202.8/385.0 MB 40.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 204.9/385.0 MB 40.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 206.9/385.0 MB 40.9 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 208.8/385.0 MB 40.9 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 210.5/385.0 MB 38.5 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 212.5/385.0 MB 38.6 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 214.5/385.0 MB 40.9 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 216.2/385.0 MB 38.6 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 217.3/385.0 MB 36.4 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 219.0/385.0 MB 34.4 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 220.7/385.0 MB 34.6 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 221.4/385.0 MB 34.4 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 223.8/385.0 MB 34.4 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 225.5/385.0 MB 32.8 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 227.2/385.0 MB 34.4 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 229.0/385.0 MB 36.4 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 230.5/385.0 MB 34.4 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 232.4/385.0 MB 36.3 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 234.3/385.0 MB 36.4 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 236.0/385.0 MB 36.4 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 237.9/385.0 MB 38.6 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 239.5/385.0 MB 36.4 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 240.9/385.0 MB 36.4 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 242.7/385.0 MB 36.4 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 244.5/385.0 MB 34.4 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 246.5/385.0 MB 36.3 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 248.4/385.0 MB 38.6 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 250.2/385.0 MB 38.6 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 251.8/385.0 MB 36.4 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 253.6/385.0 MB 38.5 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 255.5/385.0 MB 38.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 257.2/385.0 MB 38.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 258.7/385.0 MB 38.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 260.0/385.0 MB 36.3 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 262.0/385.0 MB 34.4 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 263.1/385.0 MB 32.7 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 265.0/385.0 MB 32.7 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 266.7/385.0 MB 32.7 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 268.4/385.0 MB 34.4 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 270.3/385.0 MB 34.6 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 271.8/385.0 MB 34.4 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 273.5/385.0 MB 36.4 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 275.4/385.0 MB 38.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 277.3/385.0 MB 34.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 279.1/385.0 MB 38.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 280.5/385.0 MB 36.3 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 282.2/385.0 MB 36.3 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 283.3/385.0 MB 32.8 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 285.4/385.0 MB 32.7 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 286.9/385.0 MB 34.4 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 288.6/385.0 MB 32.7 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 290.6/385.0 MB 34.4 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 292.4/385.0 MB 38.5 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 293.9/385.0 MB 36.4 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 295.8/385.0 MB 36.4 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 297.8/385.0 MB 38.5 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 299.7/385.0 MB 38.5 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 301.5/385.0 MB 38.5 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 303.3/385.0 MB 38.5 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 305.2/385.0 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 306.9/385.0 MB 38.6 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 309.0/385.0 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 310.8/385.0 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 312.4/385.0 MB 38.5 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 314.1/385.0 MB 36.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 316.1/385.0 MB 38.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 318.1/385.0 MB 40.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 320.0/385.0 MB 38.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 322.0/385.0 MB 40.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 324.0/385.0 MB 40.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 325.7/385.0 MB 38.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 327.5/385.0 MB 40.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 329.5/385.0 MB 38.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 331.4/385.0 MB 38.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 332.7/385.0 MB 36.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 334.7/385.0 MB 36.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 336.8/385.0 MB 38.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 338.5/385.0 MB 38.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 340.5/385.0 MB 38.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 342.6/385.0 MB 40.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 344.6/385.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 346.5/385.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 348.4/385.0 MB 43.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 350.3/385.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 352.1/385.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 354.2/385.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 355.6/385.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 357.5/385.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 359.4/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 361.4/385.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 363.4/385.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 365.3/385.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 367.1/385.0 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 368.7/385.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 370.6/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 372.6/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 374.4/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  376.3/385.0 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  378.2/385.0 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  380.1/385.0 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  381.8/385.0 MB 40.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  383.7/385.0 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/385.0 MB 38.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 385.0/385.0 MB 3.0 MB/s eta 0:00:00\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.5 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 51.2/57.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.5/57.5 kB 761.8 kB/s eta 0:00:00\n",
      "Downloading h5py-3.11.0-cp311-cp311-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 1.9/3.0 MB 58.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.0/3.0 MB 47.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.0/3.0 MB 47.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 21.1 MB/s eta 0:00:00\n",
      "Downloading keras-3.5.0-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.1/1.1 MB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.1/1.1 MB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 12.1 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 2.6/26.4 MB 82.2 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 4.6/26.4 MB 72.7 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 6.6/26.4 MB 52.9 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 8.3/26.4 MB 48.5 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 9.9/26.4 MB 45.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 10.4/26.4 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 10.8/26.4 MB 36.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 13.0/26.4 MB 31.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 15.6/26.4 MB 32.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 17.5/26.4 MB 32.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 19.5/26.4 MB 34.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 21.5/26.4 MB 46.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 22.1/26.4 MB 46.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 23.4/26.4 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 23.3 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.4.1-cp311-cp311-win_amd64.whl (126 kB)\n",
      "   ---------------------------------------- 0.0/126.7 kB ? eta -:--:--\n",
      "   -------------------------------------- - 122.9/126.7 kB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 126.7/126.7 kB 1.5 MB/s eta 0:00:00\n",
      "Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "   ---------------------------------------- 0.0/65.5 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/65.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 65.5/65.5 kB 891.1 kB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.5-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   --------------------------------------  409.6/413.4 kB 26.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 413.4/413.4 kB 5.2 MB/s eta 0:00:00\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.5/1.5 MB 47.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 47.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 13.5 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.12.1-cp311-cp311-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.5 kB ? eta -:--:--\n",
      "   ---------------------------------------  266.2/268.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 268.5/268.5 kB 3.3 MB/s eta 0:00:00\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, protobuf, optree, opt-einsum, ml-dtypes, h5py, google-pasta, gast, astunparse, keras, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.28.1\n",
      "    Uninstalling protobuf-5.28.1:\n",
      "      Successfully uninstalled protobuf-5.28.1\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.7.0\n",
      "    Uninstalling h5py-3.7.0:\n",
      "      Successfully uninstalled h5py-3.7.0\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 h5py-3.11.0 keras-3.5.0 libclang-18.1.1 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.3.0 optree-0.12.1 protobuf-4.25.5 tensorflow-2.17.0 tensorflow-intel-2.17.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('human_or_bot.csv')\n",
    "\n",
    "# Features and labels\n",
    "X = data['text']  # Assuming 'text' is the column with input text\n",
    "y = data['label']  # Assuming 'label' contains the labels (human or bot)\n",
    "\n",
    "# Split data into train, validation, and test sets (with stratification)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=10000)  # Use a vocabulary size of 10,000\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "max_sequence_length = 100  # You can adjust this based on the text length\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=max_sequence_length)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label distribution:\n",
      "1    700\n",
      "0    700\n",
      "Name: label, dtype: int64\n",
      "Validation label distribution:\n",
      "0    150\n",
      "1    150\n",
      "Name: label, dtype: int64\n",
      "Test label distribution:\n",
      "0    150\n",
      "1    150\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check label distribution in the train, validation, and test sets\n",
    "print(\"Train label distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"Validation label distribution:\")\n",
    "print(y_val.value_counts())\n",
    "print(\"Test label distribution:\")\n",
    "print(y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human    1000\n",
      "bot      1000\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=128, input_length=max_sequence_length))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dropout(0.5))  # Adjust dropout value later during tuning\n",
    "model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the label column contains 'human' and 'bot' as strings\n",
    "label_mapping = {'human': 0, 'bot': 1}\n",
    "\n",
    "# Apply the mapping to the labels\n",
    "y_train = y_train.map(label_mapping)\n",
    "y_val = y_val.map(label_mapping)\n",
    "y_test = y_test.map(label_mapping)\n",
    "\n",
    "# Now the labels will be 0 or 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 135ms/step - accuracy: 0.5777 - loss: 0.6632 - val_accuracy: 0.8400 - val_loss: 0.5274\n",
      "Epoch 2/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 119ms/step - accuracy: 0.8894 - loss: 0.4128 - val_accuracy: 0.8633 - val_loss: 0.3347\n",
      "Epoch 3/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 121ms/step - accuracy: 0.9544 - loss: 0.1399 - val_accuracy: 0.8867 - val_loss: 0.3406\n",
      "Epoch 4/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 136ms/step - accuracy: 0.9775 - loss: 0.1346 - val_accuracy: 0.8967 - val_loss: 0.3701\n",
      "Epoch 5/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 124ms/step - accuracy: 0.9969 - loss: 0.0196 - val_accuracy: 0.8700 - val_loss: 0.4137\n",
      "Epoch 6/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - accuracy: 0.9959 - loss: 0.0169 - val_accuracy: 0.8500 - val_loss: 0.5340\n",
      "Epoch 7/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 125ms/step - accuracy: 0.9980 - loss: 0.0193 - val_accuracy: 0.8700 - val_loss: 0.4687\n",
      "Epoch 8/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 139ms/step - accuracy: 0.9892 - loss: 0.0437 - val_accuracy: 0.8800 - val_loss: 0.5051\n",
      "Epoch 9/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 129ms/step - accuracy: 0.9953 - loss: 0.0200 - val_accuracy: 0.8667 - val_loss: 0.5414\n",
      "Epoch 10/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 128ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 0.8700 - val_loss: 0.6585\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_pad, y_train, \n",
    "                    validation_data=(X_val_pad, y_val),\n",
    "                    epochs=10,  # Adjust based on your tuning\n",
    "                    batch_size=32, \n",
    "                    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - accuracy: 1.0000 - loss: 4.0783e-05 - val_accuracy: 0.8467 - val_loss: 1.1313\n",
      "Epoch 2/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 98ms/step - accuracy: 1.0000 - loss: 4.7847e-05 - val_accuracy: 0.8467 - val_loss: 1.1386\n",
      "Epoch 3/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - accuracy: 1.0000 - loss: 2.8588e-05 - val_accuracy: 0.8467 - val_loss: 1.1485\n",
      "Epoch 4/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - accuracy: 1.0000 - loss: 2.8859e-05 - val_accuracy: 0.8467 - val_loss: 1.1639\n",
      "Epoch 5/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 1.0000 - loss: 2.3207e-05 - val_accuracy: 0.8467 - val_loss: 1.1750\n",
      "Epoch 6/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 1.0000 - loss: 2.9224e-05 - val_accuracy: 0.8467 - val_loss: 1.1857\n",
      "Epoch 7/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 106ms/step - accuracy: 1.0000 - loss: 1.9176e-05 - val_accuracy: 0.8500 - val_loss: 1.1944\n",
      "Epoch 8/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - accuracy: 1.0000 - loss: 2.6206e-05 - val_accuracy: 0.8467 - val_loss: 1.2046\n",
      "Epoch 9/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 1.0000 - loss: 2.3644e-05 - val_accuracy: 0.8467 - val_loss: 1.2162\n",
      "Epoch 10/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - accuracy: 1.0000 - loss: 2.7514e-05 - val_accuracy: 0.8433 - val_loss: 1.2279\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9031 - loss: 0.8540\n",
      "Test accuracy: 0.8899999856948853\n"
     ]
    }
   ],
   "source": [
    "# Re-run the model after addressing the data split issue\n",
    "history = model.fit(X_train_pad, y_train, \n",
    "                    validation_data=(X_val_pad, y_val),\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_pad, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8847 - loss: 0.5471\n",
      "Test Accuracy: 0.8833333253860474\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test_pad, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 142ms/step - accuracy: 0.9930 - loss: 0.0284 - val_accuracy: 0.8667 - val_loss: 0.6334\n",
      "Epoch 2/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 135ms/step - accuracy: 0.9984 - loss: 0.0120 - val_accuracy: 0.8833 - val_loss: 0.6219\n",
      "Epoch 3/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 129ms/step - accuracy: 1.0000 - loss: 5.1622e-04 - val_accuracy: 0.8733 - val_loss: 0.6942\n",
      "Epoch 4/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 130ms/step - accuracy: 1.0000 - loss: 2.2056e-04 - val_accuracy: 0.8833 - val_loss: 0.7689\n",
      "Epoch 5/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 133ms/step - accuracy: 1.0000 - loss: 1.0968e-04 - val_accuracy: 0.8833 - val_loss: 0.8705\n",
      "Epoch 6/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 122ms/step - accuracy: 1.0000 - loss: 2.1156e-04 - val_accuracy: 0.8600 - val_loss: 0.5197\n",
      "Epoch 7/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 125ms/step - accuracy: 0.9946 - loss: 0.0257 - val_accuracy: 0.7500 - val_loss: 0.5453\n",
      "Epoch 8/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 119ms/step - accuracy: 0.9733 - loss: 0.0745 - val_accuracy: 0.8567 - val_loss: 0.5699\n",
      "Epoch 9/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 122ms/step - accuracy: 0.9985 - loss: 0.0088 - val_accuracy: 0.8433 - val_loss: 0.5993\n",
      "Epoch 10/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.8600 - val_loss: 0.6269\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8789 - loss: 0.4868\n",
      "Learning Rate: 0.001, Test Accuracy: 0.8633333444595337\n",
      "Epoch 1/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 127ms/step - accuracy: 0.9959 - loss: 0.0054 - val_accuracy: 0.8400 - val_loss: 0.6579\n",
      "Epoch 2/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 1.0000 - loss: 8.7536e-04 - val_accuracy: 0.8500 - val_loss: 0.7055\n",
      "Epoch 3/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 3.1753e-04 - val_accuracy: 0.8500 - val_loss: 0.7397\n",
      "Epoch 4/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 1.6486e-04 - val_accuracy: 0.8633 - val_loss: 0.7589\n",
      "Epoch 5/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 1.0508e-04 - val_accuracy: 0.8667 - val_loss: 0.7752\n",
      "Epoch 6/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 133ms/step - accuracy: 1.0000 - loss: 9.2592e-05 - val_accuracy: 0.8600 - val_loss: 0.7927\n",
      "Epoch 7/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 124ms/step - accuracy: 1.0000 - loss: 6.7121e-05 - val_accuracy: 0.8600 - val_loss: 0.8078\n",
      "Epoch 8/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 5.1070e-05 - val_accuracy: 0.8567 - val_loss: 0.8214\n",
      "Epoch 9/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 127ms/step - accuracy: 1.0000 - loss: 3.5192e-05 - val_accuracy: 0.8600 - val_loss: 0.8300\n",
      "Epoch 10/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 123ms/step - accuracy: 1.0000 - loss: 3.7458e-05 - val_accuracy: 0.8567 - val_loss: 0.8403\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8853 - loss: 0.6919\n",
      "Learning Rate: 0.0005, Test Accuracy: 0.8733333349227905\n",
      "Epoch 1/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 133ms/step - accuracy: 1.0000 - loss: 1.9965e-05 - val_accuracy: 0.8733 - val_loss: 0.9214\n",
      "Epoch 2/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 131ms/step - accuracy: 1.0000 - loss: 7.4932e-06 - val_accuracy: 0.8767 - val_loss: 1.0704\n",
      "Epoch 3/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 114ms/step - accuracy: 1.0000 - loss: 4.8690e-06 - val_accuracy: 0.8800 - val_loss: 1.0949\n",
      "Epoch 4/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 3.0782e-06 - val_accuracy: 0.8867 - val_loss: 1.1113\n",
      "Epoch 5/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 2.7328e-06 - val_accuracy: 0.8633 - val_loss: 1.1157\n",
      "Epoch 6/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 3.9394e-06 - val_accuracy: 0.8700 - val_loss: 1.1610\n",
      "Epoch 7/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 2.0298e-06 - val_accuracy: 0.8700 - val_loss: 1.1915\n",
      "Epoch 8/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 122ms/step - accuracy: 1.0000 - loss: 1.3410e-06 - val_accuracy: 0.8767 - val_loss: 1.2358\n",
      "Epoch 9/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 123ms/step - accuracy: 1.0000 - loss: 1.1682e-06 - val_accuracy: 0.8800 - val_loss: 1.2558\n",
      "Epoch 10/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 8.4330e-07 - val_accuracy: 0.8767 - val_loss: 1.2842\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8852 - loss: 1.2515\n",
      "Learning Rate: 0.0001, Test Accuracy: 0.8733333349227905\n",
      "Epoch 1/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 133ms/step - accuracy: 1.0000 - loss: 7.5273e-07 - val_accuracy: 0.8800 - val_loss: 1.2539\n",
      "Epoch 2/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 8.1870e-07 - val_accuracy: 0.8833 - val_loss: 1.2477\n",
      "Epoch 3/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 8.1055e-07 - val_accuracy: 0.8867 - val_loss: 1.2895\n",
      "Epoch 4/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 6.7650e-07 - val_accuracy: 0.8867 - val_loss: 1.3113\n",
      "Epoch 5/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 120ms/step - accuracy: 1.0000 - loss: 4.6629e-07 - val_accuracy: 0.8900 - val_loss: 1.3023\n",
      "Epoch 6/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 131ms/step - accuracy: 1.0000 - loss: 4.9100e-07 - val_accuracy: 0.8833 - val_loss: 1.3001\n",
      "Epoch 7/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 137ms/step - accuracy: 1.0000 - loss: 5.4033e-07 - val_accuracy: 0.8833 - val_loss: 1.3382\n",
      "Epoch 8/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 4.0354e-07 - val_accuracy: 0.8867 - val_loss: 1.3658\n",
      "Epoch 9/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 4.9542e-07 - val_accuracy: 0.8867 - val_loss: 1.3710\n",
      "Epoch 10/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 4.1093e-07 - val_accuracy: 0.8867 - val_loss: 1.3798\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8687 - loss: 1.5112\n",
      "Learning Rate: 5e-05, Test Accuracy: 0.8600000143051147\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "learning_rates = [0.001, 0.0005, 0.0001, 0.00005]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_pad, y_train, epochs=10, validation_data=(X_val_pad, y_val))\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_pad, y_test)\n",
    "    print(f\"Learning Rate: {lr}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 129ms/step - accuracy: 0.6073 - loss: 0.6619 - val_accuracy: 0.8600 - val_loss: 0.5118\n",
      "Epoch 2/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 121ms/step - accuracy: 0.8886 - loss: 0.3527 - val_accuracy: 0.8700 - val_loss: 0.3131\n",
      "Epoch 3/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 126ms/step - accuracy: 0.9641 - loss: 0.1277 - val_accuracy: 0.9067 - val_loss: 0.3116\n",
      "Epoch 4/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 132ms/step - accuracy: 0.9864 - loss: 0.0516 - val_accuracy: 0.8933 - val_loss: 0.3470\n",
      "Epoch 5/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 134ms/step - accuracy: 0.9806 - loss: 0.0566 - val_accuracy: 0.8867 - val_loss: 0.3221\n",
      "Epoch 6/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 137ms/step - accuracy: 0.9781 - loss: 0.0920 - val_accuracy: 0.8733 - val_loss: 0.3489\n",
      "Epoch 7/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 135ms/step - accuracy: 0.9918 - loss: 0.0305 - val_accuracy: 0.8800 - val_loss: 0.4126\n",
      "Epoch 8/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 139ms/step - accuracy: 0.9982 - loss: 0.0159 - val_accuracy: 0.8867 - val_loss: 0.4103\n",
      "Epoch 9/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 124ms/step - accuracy: 0.9994 - loss: 0.0054 - val_accuracy: 0.8867 - val_loss: 0.5353\n",
      "Epoch 10/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 124ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.8700 - val_loss: 0.5351\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8968 - loss: 0.3829\n",
      "Dropout: 0.3, Test Accuracy: 0.8899999856948853\n",
      "Epoch 1/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.6019 - loss: 0.6667 - val_accuracy: 0.8800 - val_loss: 0.3699\n",
      "Epoch 2/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 146ms/step - accuracy: 0.9012 - loss: 0.3066 - val_accuracy: 0.8800 - val_loss: 0.3070\n",
      "Epoch 3/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 140ms/step - accuracy: 0.9624 - loss: 0.1324 - val_accuracy: 0.9067 - val_loss: 0.3394\n",
      "Epoch 4/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 140ms/step - accuracy: 0.9529 - loss: 0.1314 - val_accuracy: 0.8567 - val_loss: 0.4165\n",
      "Epoch 5/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 140ms/step - accuracy: 0.9713 - loss: 0.1172 - val_accuracy: 0.8667 - val_loss: 0.3144\n",
      "Epoch 6/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 140ms/step - accuracy: 0.9975 - loss: 0.0220 - val_accuracy: 0.8567 - val_loss: 0.5028\n",
      "Epoch 7/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 135ms/step - accuracy: 0.9962 - loss: 0.0151 - val_accuracy: 0.8633 - val_loss: 0.4278\n",
      "Epoch 8/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 135ms/step - accuracy: 0.9997 - loss: 0.0064 - val_accuracy: 0.8800 - val_loss: 0.5571\n",
      "Epoch 9/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 135ms/step - accuracy: 0.9915 - loss: 0.0336 - val_accuracy: 0.8800 - val_loss: 0.4656\n",
      "Epoch 10/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 134ms/step - accuracy: 0.9984 - loss: 0.0113 - val_accuracy: 0.8800 - val_loss: 0.4811\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8922 - loss: 0.4189\n",
      "Dropout: 0.5, Test Accuracy: 0.8833333253860474\n",
      "Epoch 1/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 127ms/step - accuracy: 0.5803 - loss: 0.6785 - val_accuracy: 0.8467 - val_loss: 0.4034\n",
      "Epoch 2/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 115ms/step - accuracy: 0.8694 - loss: 0.3582 - val_accuracy: 0.8933 - val_loss: 0.3033\n",
      "Epoch 3/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 118ms/step - accuracy: 0.9662 - loss: 0.1352 - val_accuracy: 0.8567 - val_loss: 0.3335\n",
      "Epoch 4/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 117ms/step - accuracy: 0.9890 - loss: 0.0837 - val_accuracy: 0.8800 - val_loss: 0.4222\n",
      "Epoch 5/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.9939 - loss: 0.0350 - val_accuracy: 0.8433 - val_loss: 0.8716\n",
      "Epoch 6/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 0.9778 - loss: 0.0721 - val_accuracy: 0.8867 - val_loss: 0.4356\n",
      "Epoch 7/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 0.9982 - loss: 0.0094 - val_accuracy: 0.8900 - val_loss: 0.4554\n",
      "Epoch 8/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.8867 - val_loss: 0.5865\n",
      "Epoch 9/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 110ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.8833 - val_loss: 0.4898\n",
      "Epoch 10/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.9981 - loss: 0.0150 - val_accuracy: 0.8800 - val_loss: 0.3516\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8790 - loss: 0.3740\n",
      "Dropout: 0.7, Test Accuracy: 0.8799999952316284\n"
     ]
    }
   ],
   "source": [
    "dropout_values = [0.3, 0.5, 0.7]\n",
    "\n",
    "for dp in dropout_values:\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=128, input_length=max_sequence_length))\n",
    "    model.add(LSTM(128, return_sequences=False))\n",
    "    model.add(Dropout(dp))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_pad, y_train, epochs=10, validation_data=(X_val_pad, y_val))\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_pad, y_test)\n",
    "    print(f\"Dropout: {dp}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 122ms/step - accuracy: 0.9936 - loss: 0.0348 - val_accuracy: 0.8867 - val_loss: 0.5115\n",
      "Epoch 2/5\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.9954 - loss: 0.0235 - val_accuracy: 0.8667 - val_loss: 0.6133\n",
      "Epoch 3/5\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 0.9984 - loss: 0.0031 - val_accuracy: 0.8833 - val_loss: 0.7671\n",
      "Epoch 4/5\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 1.0000 - loss: 2.6839e-04 - val_accuracy: 0.8800 - val_loss: 0.8736\n",
      "Epoch 5/5\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 1.4576e-04 - val_accuracy: 0.8700 - val_loss: 0.9312\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8658 - loss: 0.8878\n",
      "Epochs: 5, Test Accuracy: 0.8566666841506958\n",
      "Epoch 1/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 122ms/step - accuracy: 0.9911 - loss: 0.0923 - val_accuracy: 0.8500 - val_loss: 0.5423\n",
      "Epoch 2/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.9983 - loss: 0.0069 - val_accuracy: 0.8567 - val_loss: 0.7453\n",
      "Epoch 3/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 0.9968 - loss: 0.0224 - val_accuracy: 0.8433 - val_loss: 0.8473\n",
      "Epoch 4/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.9826 - loss: 0.0548 - val_accuracy: 0.7733 - val_loss: 0.5184\n",
      "Epoch 5/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.9857 - loss: 0.0590 - val_accuracy: 0.7600 - val_loss: 0.5895\n",
      "Epoch 6/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.9855 - loss: 0.0483 - val_accuracy: 0.8733 - val_loss: 0.6988\n",
      "Epoch 7/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 0.9987 - loss: 0.0029 - val_accuracy: 0.8733 - val_loss: 0.6591\n",
      "Epoch 8/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.8733 - val_loss: 0.6939\n",
      "Epoch 9/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 1.0000 - loss: 5.5489e-04 - val_accuracy: 0.8800 - val_loss: 0.7506\n",
      "Epoch 10/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 1.0000 - loss: 4.5283e-04 - val_accuracy: 0.8867 - val_loss: 0.7939\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8568 - loss: 0.8628\n",
      "Epochs: 10, Test Accuracy: 0.8666666746139526\n",
      "Epoch 1/15\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 119ms/step - accuracy: 0.9949 - loss: 0.0476 - val_accuracy: 0.8600 - val_loss: 0.7303\n",
      "Epoch 2/15\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 1.0000 - loss: 3.4988e-04 - val_accuracy: 0.8800 - val_loss: 0.8480\n",
      "Epoch 3/15\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 1.9385e-04 - val_accuracy: 0.8900 - val_loss: 0.8285\n",
      "Epoch 4/15\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 1.0000 - loss: 1.2205e-04 - val_accuracy: 0.8933 - val_loss: 0.8309\n",
      "Epoch 5/15\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 122ms/step - accuracy: 1.0000 - loss: 9.4702e-05 - val_accuracy: 0.9000 - val_loss: 0.8508\n",
      "Epoch 6/15\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 1.0000 - loss: 6.4309e-05 - val_accuracy: 0.9000 - val_loss: 0.8677\n",
      "Epoch 7/15\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 6.1127e-05 - val_accuracy: 0.9033 - val_loss: 0.8928\n",
      "Epoch 8/15\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 1.0000 - loss: 3.8298e-05 - val_accuracy: 0.9000 - val_loss: 0.9070\n",
      "Epoch 9/15\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 1.0000 - loss: 4.3273e-05 - val_accuracy: 0.8967 - val_loss: 0.9397\n",
      "Epoch 10/15\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 1.0000 - loss: 3.1166e-05 - val_accuracy: 0.9000 - val_loss: 0.9843\n",
      "Epoch 11/15\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 1.0000 - loss: 2.6725e-05 - val_accuracy: 0.9000 - val_loss: 1.0223\n",
      "Epoch 12/15\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 2.6647e-05 - val_accuracy: 0.9000 - val_loss: 1.0513\n",
      "Epoch 13/15\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 2.0675e-05 - val_accuracy: 0.9000 - val_loss: 1.0731\n",
      "Epoch 14/15\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 1.7912e-05 - val_accuracy: 0.8967 - val_loss: 1.0942\n",
      "Epoch 15/15\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 1.0000 - loss: 1.3413e-05 - val_accuracy: 0.8967 - val_loss: 1.1136\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8571 - loss: 1.2205\n",
      "Epochs: 15, Test Accuracy: 0.8399999737739563\n",
      "Epoch 1/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 119ms/step - accuracy: 0.9984 - loss: 0.0152 - val_accuracy: 0.8067 - val_loss: 1.1028\n",
      "Epoch 2/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 1.0000 - loss: 7.6685e-04 - val_accuracy: 0.8267 - val_loss: 1.2487\n",
      "Epoch 3/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 1.0000 - loss: 8.7871e-05 - val_accuracy: 0.8433 - val_loss: 1.4055\n",
      "Epoch 4/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 1.0000 - loss: 3.7231e-05 - val_accuracy: 0.8467 - val_loss: 1.3898\n",
      "Epoch 5/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - accuracy: 0.9994 - loss: 0.0019 - val_accuracy: 0.7567 - val_loss: 1.0493\n",
      "Epoch 6/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 118ms/step - accuracy: 0.9939 - loss: 0.0203 - val_accuracy: 0.8133 - val_loss: 0.7901\n",
      "Epoch 7/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 0.9970 - loss: 0.0129 - val_accuracy: 0.8200 - val_loss: 0.8909\n",
      "Epoch 8/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 120ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.8433 - val_loss: 0.8913\n",
      "Epoch 9/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 1.0000 - loss: 2.3903e-04 - val_accuracy: 0.8467 - val_loss: 0.9214\n",
      "Epoch 10/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 1.0000 - loss: 2.2126e-04 - val_accuracy: 0.8367 - val_loss: 0.9577\n",
      "Epoch 11/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 1.0000 - loss: 1.3699e-04 - val_accuracy: 0.8400 - val_loss: 0.9816\n",
      "Epoch 12/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 1.0000 - loss: 1.1106e-04 - val_accuracy: 0.8400 - val_loss: 1.0024\n",
      "Epoch 13/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 1.0000 - loss: 8.7462e-05 - val_accuracy: 0.8433 - val_loss: 1.0195\n",
      "Epoch 14/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 1.0000 - loss: 1.0839e-04 - val_accuracy: 0.8467 - val_loss: 1.0342\n",
      "Epoch 15/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 1.0000 - loss: 9.5907e-05 - val_accuracy: 0.8467 - val_loss: 1.0558\n",
      "Epoch 16/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 1.0000 - loss: 1.1518e-04 - val_accuracy: 0.8467 - val_loss: 1.0715\n",
      "Epoch 17/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 1.0000 - loss: 4.7013e-05 - val_accuracy: 0.8467 - val_loss: 1.0827\n",
      "Epoch 18/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - accuracy: 1.0000 - loss: 3.8179e-05 - val_accuracy: 0.8467 - val_loss: 1.0950\n",
      "Epoch 19/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 1.0000 - loss: 6.3050e-05 - val_accuracy: 0.8467 - val_loss: 1.1096\n",
      "Epoch 20/20\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 108ms/step - accuracy: 1.0000 - loss: 4.4774e-05 - val_accuracy: 0.8467 - val_loss: 1.1210\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9078 - loss: 0.7393\n",
      "Epochs: 20, Test Accuracy: 0.8999999761581421\n"
     ]
    }
   ],
   "source": [
    "epochs_values = [5, 10, 15, 20]\n",
    "\n",
    "for epochs in epochs_values:\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_pad, y_train, epochs=epochs, validation_data=(X_val_pad, y_val))\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_pad, y_test)\n",
    "    print(f\"Epochs: {epochs}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentLSTM(\n",
      "  (embedding): Embedding(55951, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch: 1/4... Loss: 0.6050\n",
      "Epoch: 2/4... Loss: 0.4973\n",
      "Epoch: 3/4... Loss: 0.3049\n",
      "Epoch: 4/4... Loss: 0.2206\n",
      "Validation Loss: 0.5450... Validation Accuracy: 0.7633\n",
      "Test Loss: 0.6707... Test Accuracy: 0.6967\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import Counter\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('human_or_bot.csv')\n",
    "texts = data['text'].values  # Assume 'text' is the column name\n",
    "labels = data['label'].values  # Assume 'label' is the column name (0 for human, 1 for bot)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(labels)\n",
    "\n",
    "# Tokenization and vocabulary creation\n",
    "tokenized_texts = [text.lower().split() for text in texts]\n",
    "counter = Counter(word for text in tokenized_texts for word in text)\n",
    "vocab = {word: i + 1 for i, (word, _) in enumerate(counter.items())}  # +1 for padding\n",
    "\n",
    "# Convert texts to sequences\n",
    "max_length = 100  # Maximum length of the sequences\n",
    "X = []\n",
    "for text in tokenized_texts:\n",
    "    seq = [vocab[word] for word in text if word in vocab]\n",
    "    if len(seq) < max_length:\n",
    "        seq += [0] * (max_length - len(seq))  # Padding\n",
    "    else:\n",
    "        seq = seq[:max_length]  # Truncate\n",
    "    X.append(seq)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y_encoded)\n",
    "\n",
    "# Train/test/validation split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_valid = torch.tensor(X_valid, dtype=torch.long)\n",
    "y_valid = torch.tensor(y_valid, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Define the LSTM model\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.dropout(lstm_out[:, -1])\n",
    "        out = self.fc(out)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "# Instantiate model\n",
    "vocab_size = len(vocab) + 1  # +1 for padding\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "dropout_prob = 0.5\n",
    "\n",
    "model = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout_prob)\n",
    "print(model)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "epochs = 4\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs).squeeze()\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {e+1}/{epochs}... Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    valid_output = model(X_valid).squeeze()\n",
    "    valid_loss = criterion(valid_output, y_valid)\n",
    "    valid_acc = ((valid_output > 0.5).float() == y_valid).float().mean()\n",
    "    print(f'Validation Loss: {valid_loss.item():.4f}... Validation Accuracy: {valid_acc.item():.4f}')\n",
    "\n",
    "# Test evaluation\n",
    "with torch.no_grad():\n",
    "    test_output = model(X_test).squeeze()\n",
    "    test_loss = criterion(test_output, y_test)\n",
    "    test_acc = ((test_output > 0.5).float() == y_test).float().mean()\n",
    "    print(f'Test Loss: {test_loss.item():.4f}... Test Accuracy: {test_acc.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6707... Test Accuracy: 0.6967\n"
     ]
    }
   ],
   "source": [
    "print(f'Test Loss: {test_loss.item():.4f}... Test Accuracy: {test_acc.item():.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1aD1Kmy35cHnPkvtrTbtJL9Ohjg7XRzYV",
     "timestamp": 1726514743356
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
